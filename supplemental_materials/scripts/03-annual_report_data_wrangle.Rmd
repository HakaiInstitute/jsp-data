---
title: "data wrangle for Hakai Juvenile Salmon Program Time Series"
author: "Brett Johnson"
date: "October 16, 2018"
output: html_document
---

# Summary

This script is meant to read in the tidy data from the Hakai Juvenile Salmon Program, and filters data to something that permits inter-annual comparison of the key migration variables. 

This script reads from tidy_data and writes to report_data and creates the tables that are used in our annual report for the North Pacific Anadromous Fisheries Commission Document.

The key migration variables are migration timing, fish length, sea lice, catch intensity, species proportions, and 30 m depth integrated ocean temperatures. All these parameters are then plotted in a heatmap to provide a snapshot of each seasons variables in comparison to previous years as a hypothesis generating tool.


```{r setup}
library(tidyverse)
library(lubridate)
library(knitr)
library(here)
library(car)

# I use this function to produce nice looking dates for plotting
f <- function(x) {
  format(x + as.Date("2019-01-01") -1, format = "%B %d")
}

# These are the sites that are consistent between years and permit inter-annual comparison
consistent_sites <-  c("D07","D09","D22","D27","D10","D08","D34","D35",
                       "D20","J03","J02","J09","J11")

sites <- read_csv(here("supplemental_materials", "tidy_data", "sites.csv"))

survey_data <- read_csv(here("supplemental_materials", "tidy_data", "survey_data.csv")) %>% 
  left_join(sites, by = 'site_id')

seine_data <- read_csv(here("supplemental_materials", "tidy_data", "seine_data.csv"))

survey_seines <- left_join(survey_data, seine_data, by = "survey_id") %>% 
  mutate(year = year(survey_date),
         yday = yday(survey_date)) %>%
  # remove ad-hoc collections from migration timing calcs & sampling events from normal sampling period (May 1 - July 9) and only select sites that were collected in every year, or at least represent the various regions in a similar way
  filter(
    collection_protocol == "SEMSP",
    set_type == "targeted",
    yday < 190,
    site_id %in% consistent_sites
    )

write_csv(survey_seines, here::here("supplemental_materials", "report_data", "survey_seines.csv"))
saveRDS(survey_seines,  here::here("supplemental_materials", "report_data", "survey_seines.RDS"))

fish_field_data <- read_csv(here("supplemental_materials", "tidy_data", "fish_field_data.csv"), guess_max = 20000)

survey_seines_fish <- left_join(survey_seines, fish_field_data, by = "seine_id") %>% 
  drop_na(ufn) # drops seines for which there were no fish associated

sealice_time_series <- read_csv(here("supplemental_materials", "tidy_data", "combined_motile_lice.csv")) %>%
   gather(`motile_caligus`,
         `motile_lep`,
         `all_lice`,
         key = louse_species,
         value = n_lice) %>%
  mutate(year = year(survey_date)) %>% 
  drop_na() %>%
  filter(species %in% c("SO", "PI", "CU")) %>% 
  left_join(sites, by =  "site_id") %>% 
  select(-c(site_name, zone, site_priority, pfma, site_notes, survey_start_lat,
            survey_start_lon, survey_end_lat, survey_end_lon, ocgy_std_lat, 
            ocgy_std_lon))

write_csv(sealice_time_series, here("supplemental_materials", "report_data", "sealice_time_series.csv"))
saveRDS(sealice_time_series,  here::here("supplemental_materials", "report_data", "sealice_time_series.RDS"))

current_year <- max(survey_seines$year, na.rm = T)
project_years <- current_year - 2015 + 1
study_range <- paste(2015, "-", current_year)

```


```{r sealice abundance, cache=TRUE}
library(rsample)
library(dplyr)
library(tidyr)
library(broom)
library(purrr)
library(modelr)

mean_n_lice <- function(splits) {
  x <- analysis(splits)
  mean(x$n_lice)
}

sealice_time_series <- sealice_time_series |> 
    filter(
    site_id %in% consistent_sites
    )

# this_is_it correctly nests the data into its hierarchical form
this_is_it <- sealice_time_series %>% 
  filter(louse_species != "all_lice") %>% 
  base::split(list(.$year, .$region, .$site_id, .$survey_date, .$species, .$louse_species))

# remove empty permutations
row_lt1 <- which(sapply(this_is_it, nrow) < 1)
this_is_it <- this_is_it[-row_lt1]

# Written by Biljana
boots <- lapply(1:length(this_is_it), function(x){bootstraps(this_is_it[[x]], times=1000)})
 
abund_boot_samples <- lapply(1:length(boots),function(x){  
  boots[[x]] %>% 
  mutate(samples = unlist(map(splits, mean_n_lice)))})

extract.boot.CI <- lapply(1:length(abund_boot_samples), function(x){quantile(abund_boot_samples[[x]]$samples,c(0.025, 0.975))} )

extract.boot.CI = do.call(rbind,extract.boot.CI)

# Back to Brett's code
extract.boot.CI <- as_tibble(extract.boot.CI) 
extract.boot.CI$estimate <- (extract.boot.CI$`97.5%` + extract.boot.CI$`2.5%`) / 2
category <- names(this_is_it)
extract.boot.CI$category <- category

lice_bootstraps <- extract.boot.CI %>% 
separate(category, into = c("year", "region", "site_id", "year2", "month", "day", "species", "stage", "louse_species")) %>% 
  select(-year, year = year2)

readr::write_csv(lice_bootstraps, here("supplemental_materials", "report_data", "lice_bootstraps.csv"))
saveRDS(lice_bootstraps,  here::here("supplemental_materials", "report_data", "lice_bootstraps.RDS"))

```

```{r Catch Intensity}
# In 2015 and 2016 we only enumerated seines with sockeye. In 2017 onwards we enumerated catch of all seines even if they didn't have sockeye. This created a problem because from those first two years of the program we only have pink and chum abundance measurements when they were caught with sockeye. So the only inter-annual comparable abundance parameter across years is 'intensity'â€” the number of sockeye, pink, and chum that were caught when greater than zero sockeye were caught.

catch_intensity <- survey_seines %>% 
  filter(region == "DI") %>% 
  rename("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total") %>%
  # remove seines that did not catch any sockeye, even if they caught other spp
  filter(Sockeye > 0) %>% 
  select(year, Sockeye, Pink, Chum) %>% 
  gather(key = species, value = catch, - year) %>% 
  # Filter out catches where no pink and chum were caught, so that catch intensity is consistent for each species. Ie. catch intensity for sockeye is the average number of sockeye when catch of sockeye is > 0, catch intensity of pink is the average number of pink caught when > 1 pink was caught, and so on for chum as well.
  filter(catch > 0)

catch_intensity <- catch_intensity %>% 
  group_by(year, species) %>% 
  summarize(mean_catch = mean(catch),
            sd = sd(catch),
            n = n())%>% 
  mutate(se = sd / sqrt(n),
         lower_ci = qt(1 - (0.05 / 2), n - 1) * se,
         upper_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 1)

write_csv(catch_intensity, here::here("supplemental_materials", "report_data", "catch_intensity.csv"))
saveRDS(catch_intensity,  here::here("supplemental_materials", "report_data", "catch_intensity.RDS"))
```

```{r Migration Timing}

# Take the average of surveys with double effort in 2019

dbl_effort_2019 <- survey_seines %>% 
  distinct(survey_id, .keep_all = TRUE) %>% 
  filter(survey_date > as.Date("2019-01-01") & survey_date < ("2019-12-31")) %>% 
  mutate(week = isoweek(survey_date),
         site_week = paste(site_id,week,sep="-")) %>% 
  group_by(site_week) %>% 
  mutate(count=n()) %>% 
  filter(count>1) %>% 
  mutate_at(c("so_total", "pi_total", "cu_total", "co_total", "he_total"),
            mean) %>% 
  mutate_at(c("so_total", "pi_total", "cu_total", "co_total", "he_total"),
            round)

# I first take Cumulative abundance
tidy_catch <- survey_seines %>%
  # Filter out the double-effort surveys
  filter(!survey_id %in% dbl_effort_2019$survey_id)

dbl_effort_2019 <- dbl_effort_2019 %>% 
  # Remove secondary surveys so only one record remains that contains the averaged catch values
  distinct(site_week, .keep_all = TRUE) %>% 
  ungroup() %>% 
  select(-site_week)

tidy_catch <- tidy_catch %>% 
  bind_rows(dbl_effort_2019) %>% 
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # filter out instances when sockeye were not caught because in 2015 we only enumerated sets that had sockeye in them, and moving forward we plan on enumerating all sets. If we included sets that had zero sockeye in 2017 and 2018 then our CPUE of sockeye in 2015 and 2016 would be biased high. Therefore these abundance metrics are strictly based on seines which had sockeye and we are then measuring how many of each species are in a seine, from seines with sockeye, as our metric of all species abundance and proportion. This is the only way to resolve inconsistent sampling strategies between years.
  filter(so_total > 0) %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

tidy_catch <- as.data.frame(tidy_catch)

tidy_catch <- tidy_catch %>%
  mutate(yday = yday(survey_date))

# TIME SERIES AVERAGES

# Create a table with time series average daily proportion for each species and each region for all years combined 

#Sockeye
so_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "so_total", year != current_year) %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

so_total_catch_expanded_cum_DI <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "DI") |> 
  ungroup()

so_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    so_total_catch_expanded_cum_DI$percent,
    so_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "so_total") %>%
  mutate(year = paste("2015 -", current_year - 1)) %>%
  mutate(daily_percent = y - lag(y))

#Pink
pi_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "pi_total", year != current_year) %>%
  # figure out if a year is odd or even and filter out odd years
  mutate(even_or_odd = year %% 2) %>% 
  filter(even_or_odd != 1) %>% 
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

pi_total_catch_expanded_cum_DI <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

pi_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    pi_total_catch_expanded_cum_DI$percent,
    pi_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "pi_total") %>%
  mutate(year = paste("2015 -",  current_year - 1)) %>%
  mutate(daily_percent = y - lag(y))

# Chum
cu_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "cu_total", year != current_year) %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

cu_total_catch_expanded_cum_DI <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

cu_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    cu_total_catch_expanded_cum_DI$percent,
    cu_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "cu_total") %>%
  mutate(year = paste("2015 -", current_year - 1)) %>%
  mutate(daily_percent = y - lag(y))

# Create table of time series average migration timing for DI
predict_average_prop <-
  rbind(
    pi_total_predict_average_prop_DI,
    so_total_predict_average_prop_DI,
    cu_total_predict_average_prop_DI
  ) %>%
  drop_na(daily_percent)

predict_average_prop$species <- factor(predict_average_prop$species) %>% 
  fct_recode("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total")

write_csv(predict_average_prop,
          here::here("supplemental_materials", "report_data", "predict_average_prop.csv"))
saveRDS(predict_average_prop, here::here("supplemental_materials", "report_data", "predict_average_prop.RDS"))

# ANNUAL CUMULATIVE ABUNDANCE CURVES FOR EACH SPECIES

# Discovery Islands only, not doing this for Johnstone Strait
daily_mean_cumul_abund_annual <- tidy_catch %>%
  ungroup() %>%
  filter(region == "DI") %>%
  mutate(year = as.factor(year)) %>%
  group_by(year, region, species) %>%
  arrange(year, region, species, yday) %>% 
  mutate(cumsum = cumsum(n),
          percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    survey_date = yday,
    region = region,
    species = species,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  group_by(year, region, species, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

#2015 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2015)

so_predict_annual_prop_DI_2015 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2015$percent,
                  so_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# chum
cu_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2015)

cu_predict_annual_prop_DI_2015 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2015$percent,
                  cu_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2016 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2016)

so_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2016$percent,
                  so_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2016)

pi_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2016$percent,
                  pi_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2016)

cu_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2016$percent,
                  cu_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2017 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2017)

so_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2017$percent,
                  so_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "SO")


## chum
cu_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2017)
          
cu_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
                  cu_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

cu_cum_abund_annual_DI_2017$species <- "Chum"

# For now, I'll fjust use raw points without a line
# TODO: Consider removing this data frame, depending on further model selection
write_csv(cu_cum_abund_annual_DI_2017, here("supplemental_materials", "report_data", "cu_cum_abund_DI_2017.csv"))
saveRDS(cu_cum_abund_annual_DI_2017, here::here("supplemental_materials", "report_data", "cu_cum_abund_annual_DI_2017.RDS"))          

cu_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
                  cu_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

cu_cum_abund_2017 <- cu_cum_abund_annual_DI_2017 %>% 
  ungroup() %>% 
  mutate(species = "Chum")

# For now, I'll fjust use raw points without a line
write_csv(cu_cum_abund_2017, here("supplemental_materials", "report_data", "cu_cum_abund_2017.csv"))
saveRDS(cu_cum_abund_2017, here::here("supplemental_materials", "report_data", "cu_cum_abund_2017.RDS"))

#2018 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2018)

so_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2018$percent,
                  so_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2018)

pi_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2018$percent,
                  pi_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2018)

cu_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2018$percent,
                  cu_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2019 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2019)

so_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2019$percent,
                  so_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2019)

pi_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2019$percent,
                  pi_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2019)

cu_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2019$percent,
                  cu_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2020 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2020)

so_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2020$percent,
                  so_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2020)

pi_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2020$percent,
                  pi_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2020)

cu_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2020$percent,
                  cu_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2021 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2021 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2021)

so_predict_annual_prop_DI_2021 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2021$percent,
                  so_cum_abund_annual_DI_2021$survey_date) %>%
  mutate(region = "DI", year = 2021) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2021 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2021)

pi_predict_annual_prop_DI_2021 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2021$percent,
                  pi_cum_abund_annual_DI_2021$survey_date) %>%
  mutate(region = "DI", year = 2021) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2021 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2021)

cu_predict_annual_prop_DI_2021 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2021$percent,
                  cu_cum_abund_annual_DI_2021$survey_date) %>%
  mutate(region = "DI", year = 2021) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

# Put each years modelled cumulative abundnance into one table
predict_annual_prop <-
  rbind(
    so_predict_annual_prop_DI_2015,
    cu_predict_annual_prop_DI_2015,
    so_predict_annual_prop_DI_2016,
    pi_predict_annual_prop_DI_2016,
    cu_predict_annual_prop_DI_2016,
    so_predict_annual_prop_DI_2017,
    cu_predict_annual_prop_DI_2017, 
    so_predict_annual_prop_DI_2018,
    pi_predict_annual_prop_DI_2018,
    cu_predict_annual_prop_DI_2018,
    so_predict_annual_prop_DI_2019,
    pi_predict_annual_prop_DI_2019,
    cu_predict_annual_prop_DI_2019,
    so_predict_annual_prop_DI_2020,
    pi_predict_annual_prop_DI_2020,
    cu_predict_annual_prop_DI_2020,
    so_predict_annual_prop_DI_2021,
    pi_predict_annual_prop_DI_2021,
    cu_predict_annual_prop_DI_2021
  )

predict_annual_prop$species <- factor(predict_annual_prop$species) %>% 
  fct_recode("Sockeye" = "SO", "Pink" = "PI", "Chum" = "CU") 

colnames(predict_annual_prop)[1] <- "x"

write_csv(predict_annual_prop, here::here("supplemental_materials", "report_data", "predict_annual_prop.csv"))
saveRDS(predict_annual_prop, here::here("supplemental_materials", "report_data", "predict_annual_prop.RDS"))

# Create data table for real (not modelled), q1, q2, q3 migration timing stats
peak_dates <- tidy_catch[rep(row.names(tidy_catch), tidy_catch$n), 1:6] %>%
  filter(species %in% c("so_total", "pi_total", "cu_total")) %>%
  mutate(yday = yday(survey_date)) %>%
  group_by(year, region, species) %>%
  summarise(n = n(), q1 = quantile(yday, probs = 0.25), q3 = quantile(yday, probs = 0.75), median = median(yday)) %>%
  ungroup() %>% 
  mutate(species = replace(species, species == "so_total", "SO")) %>%
  mutate(species = replace(species, species == "pi_total", "PI")) %>%
  mutate(species = replace(species, species == "cu_total", "CU")) %>% 
  mutate(Spread = median - q1) %>% 
  mutate(q1 = ifelse(species == "PI" & year %% 2 == 1, NA, q1),
         median = ifelse(species == "PI" & year %% 2 == 1, NA, median),
         q3 = ifelse(species == "PI" & year %% 2 == 1, NA, q3),
         Spread = ifelse(species == "PI" & year %% 2 == 1, NA, Spread)) %>% 
    mutate(year = as.character(year))

tsa_peak_dates <- peak_dates %>%
  ungroup() %>% 
  group_by(region, species) %>% 
  summarise(n = mean(n, na.rm = TRUE),
            q1 = mean(q1, na.rm = TRUE),
            median = mean(median, na.rm = TRUE),
            q3 = mean(q3, na.rm = TRUE),
            Spread = mean(Spread, na.rm = TRUE)
            ) %>% 
  mutate(year = ifelse(region == "DI" ,paste(2015, "-", current_year), "2015 - 2019")) %>% 
  select(year, everything())

peak_dates <- bind_rows(tsa_peak_dates, peak_dates)

write_csv(peak_dates, here("supplemental_materials", "report_data","peak_dates.csv"))
saveRDS(peak_dates, here::here("supplemental_materials", "report_data", "peak_dates.RDS"))
```

```{r Lengths and Condition}

fish_lab_data <- read_csv(here("supplemental_materials", "tidy_data", "fish_lab_data.csv"), guess_max = 10000)

fish_data <- left_join(survey_seines_fish, fish_lab_data, by = "ufn") %>%
  # combine both fork length measurements
  mutate(
    fork_length_lab = as.numeric(fork_length),
    fork_length_field = as.numeric(fork_length_field),
    fork_length = coalesce(fork_length_lab, fork_length_field)
  )

length_histo <- fish_data %>%
  select(survey_date, region, species, fork_length) %>%
  drop_na(fork_length) %>%
  mutate(year = year(survey_date)) %>%
  # Remove incidentally sampled species
  filter(species != "CK", species != "CO", species != "HE", region != "JS") %>%
  mutate(year = as.factor(year))

saveRDS(length_histo, here::here("supplemental_materials", "report_data", "length_histo.RDS"))
write_csv(length_histo, here::here("supplemental_materials", "report_data", "length_histo.csv"))

fish_cond <- fish_data %>% 
  filter(region == "DI") %>% 
  select(survey_date, species, weight, fork_length) %>% 
  filter(species %in% c('SO', 'PI', 'CU', 'CO')) %>% 
  mutate(year = factor(year(survey_date))) 

write_csv(fish_cond, here("supplemental_materials", "report_data", "fish_cond.csv"))
saveRDS(fish_cond, here("supplemental_materials", "report_data", "fish_cond.RDS"))
```

```{r Species Proportions}

spp_prop <- survey_seines %>%
  filter(region == "DI") %>% 
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # Next I remove instances when no sockeye were caught. I'm doing this because in 2015 and 2016 we only enumerated catches in which we caught sockeye, whereas in 2017 and 2018 we enumerated all seines. So to reduce the bias introduced from the field method i filter the comparison down to where field methods are comparable
  filter(so_total > 0) %>%
  # remove instances when not all species were enumerated by droping rows with NA
  drop_na() %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

spp_prop_expanded <-
  spp_prop[rep(row.names(spp_prop), spp_prop$n), 1:6] %>%
  mutate(yday = yday(survey_date), year = year(survey_date))

proportions <- spp_prop_expanded %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n)) %>%
  ungroup()

write_csv(proportions, here::here("supplemental_materials", "report_data", "proportion.csv"))
saveRDS(proportions, here::here("supplemental_materials", "report_data", "proportion.RDS"))


```


```{r SST}

qu39_ctd <- read_csv(here("supplemental_materials", "tidy_data", "qu39_ctd.csv"))

qu39 <- qu39_ctd %>% 
  select(
    year,
    date,
    week,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, date, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  ) %>% 
  ungroup()

write_csv(qu39, here::here("supplemental_materials", "report_data", "qu39_ctd.csv"))
saveRDS(qu39, here::here("supplemental_materials", "report_data", "qu39_ctd.RDS"))


# SST ANOMALY DATA
## Create current year data to compare to time series which excludes the current year for comparison purposes

qu39_this_year <- qu39 %>%
  ungroup() %>% 
  filter(year(date) == current_year) %>% 
  group_by(yday) %>% 
  summarize(mean_temp = mean(mean_temp),
            mean_do = mean(mean_do),
            mean_salinity = mean(mean_salinity))

qu39_average <- qu39 %>%
  filter(year(date) != current_year)

# Here I generate a loess regression for the average temperature based on the time series. Loess and the span variable are both rather arbitrary and more of a statistical convenience than perhaps a more rigorous model?
temp.lo_qu39 <-
  loess(mean_temp ~ yday, qu39_average, SE = T, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu39 <-
  tibble(yday = seq(min(qu39_average$yday), max(qu39_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu39$predicted_mean_temp <-
  predict(temp.lo_qu39, sim_temp_data_qu39, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
#### #TEST automated function:
library(purrr)
# Function to calculate the midpoint row
calc_midpoint_row <- function(df, i) {
  yday = (df$yday[i] + df$yday[i+1]) / 2
  predicted_mean_temp = (df$predicted_mean_temp[i] + 
                           df$predicted_mean_temp[i+1]) / 2
  mean_temp = predicted_mean_temp
  return(data.frame(yday = yday, predicted_mean_temp = predicted_mean_temp, mean_temp = mean_temp))
}

# Generate the initial data
qu39_temp_anomaly_data <- left_join(sim_temp_data_qu39, qu39_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff)

# Generate mid-point rows
mid_points <- map_dfr(1:(nrow(qu39_temp_anomaly_data) - 1), ~ {
  if(qu39_temp_anomaly_data$diff[.x] != qu39_temp_anomaly_data$diff[.x + 1]) {
    calc_midpoint_row(qu39_temp_anomaly_data, .x)
  }
})

# Combine original data with the new mid-point rows
qu39_temp_anomaly_data <- bind_rows(qu39_temp_anomaly_data, mid_points) %>%
  arrange(yday)  # make sure data is in order by yday

# See https://stackoverflow.com/questions/27135962/how-to-fill-geom-polygon-with-different-colors-above-and-below-y-0
 
# Create min and max temp value for any given week of the time series
qu39_min_max <- qu39_ctd %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature), week = week) %>%
  ungroup() %>%
  group_by(week) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU39")



min_max_data <- qu39_min_max
saveRDS(min_max_data,  here::here("supplemental_materials", "report_data", "min_max_temps.RDS"))
write_csv(min_max_data,  here::here("supplemental_materials", "report_data", "min_max_temps.csv"))

average_temps <- qu39_average
saveRDS(average_temps, here::here("supplemental_materials", "report_data", "average_temps.RDS"))
write_csv(average_temps, here::here("supplemental_materials", "report_data", "average_temps.csv"))

temperature_anomaly_data <- qu39_temp_anomaly_data
saveRDS(temperature_anomaly_data,
        here::here("supplemental_materials", "report_data", "temperature_anomaly_data.RDS"))
write_csv(temperature_anomaly_data,
          here::here("supplemental_materials", "report_data", "temperature_anomaly_data.csv"))

```

# Heat Map

The idea of the heatmap is to provide one plot that broadly characterizes the variability, or the deviations from the time series average, for each migration parameter. This is done using a z-score which can be thought of as the number of standard deviations away from the mean. This assumes normality in the distribution of means for each annually averaged variable (peak migration date, length, etc). Distributions are checked for normality and if not normal, a non-parametric z score is used.

```{r Heatmap}
library(ggpubr)
library(rcompanion)

# Here I test each set of annual observations for the assumption of normality. I use  the shapiro test for normality that has a null hypothesis that the data  are normally distributed. Thus, if p < 0.05 then the data are not normal and a transformation must be made. Each year, look at the p-values and determine if data need to be transformed.

# Migration Timing z-scores

# Sockeye migrations
so_peak_dates <- peak_dates %>% 
  filter(species == "SO", region == "DI", year != study_range) 

ggdensity(so_peak_dates$median)
so_shap_t <- shapiro.test(so_peak_dates$median)
#sockeye peak dates are definitely not normally distributed

#transform using inverse normal transformation
trans_so_peak <- blom(so_peak_dates$median)
ggdensity(trans_so_peak)

so_peak_dates <- so_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = trans_so_peak)

so_migration_dates_z <- so_peak_dates$z_score

# Pink migrations
PI_peak_dates <- peak_dates %>% 
  filter(species == "PI", region == "DI", year != study_range) 

ggdensity(PI_peak_dates$median)
shapiro.test(PI_peak_dates$median)

PI_mean_of_median_date <- mean(PI_peak_dates$median, na.rm = TRUE)
PI_sd_of_median_date <- sd(PI_peak_dates$median, na.rm = TRUE)

 
PI_peak_dates <- PI_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - PI_mean_of_median_date) / PI_sd_of_median_date)

PI_migration_dates_z <- PI_peak_dates$z_score

# Chum migrations
CU_peak_dates <- peak_dates %>% 
  filter(species == "CU", region == "DI", year != study_range) 

ggdensity(CU_peak_dates$median)
shapiro.test(CU_peak_dates$median)

CU_mean_of_median_date <- mean(CU_peak_dates$median)
CU_sd_of_median_date <- sd(CU_peak_dates$median)

CU_peak_dates <- CU_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - CU_mean_of_median_date) / CU_sd_of_median_date)

CU_migration_dates_z <- CU_peak_dates$z_score

# Length z-scores

# Sockeye lengths
so_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "SO")

#Test for normality
ggdensity(so_mean_sd$mean)
shapiro.test(so_mean_sd$mean)

#calculate  time series average length
so_avg_length <- as.numeric(so_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

#calculate time series standard deviation 
so_sd_length <- as.numeric(so_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

so_length_z <- (so_mean_sd$mean - so_avg_length[2]) / so_sd_length[2]

# Pink lengths
PI_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "PI")

ggdensity(PI_mean_sd$mean)
shapiro.test(PI_mean_sd$mean)

PI_avg_length <- as.numeric(PI_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

PI_sd_length <- as.numeric(PI_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

PI_length_z <- (PI_mean_sd$mean - PI_avg_length[2]) / PI_sd_length[2]

# Chum lengths
CU_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "CU")

ggdensity(CU_mean_sd$mean)
shapiro.test(CU_mean_sd$mean)

CU_avg_length <- as.numeric(CU_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

CU_sd_length <- as.numeric(CU_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

CU_length_z <- (CU_mean_sd$mean - CU_avg_length[2]) / CU_sd_length[2]

# Sealice z-scores
# Remove sealice observations from JS 
lice_bootstraps <- lice_bootstraps %>%
  filter(region != "JS")

# Sockeye sealice
so_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "SO") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance),
            ) %>% 
  mutate_if(is.numeric, round, 2)

ggdensity(so_api_annual_mean$Abundance)
shapiro.test(so_api_annual_mean$Abundance)

so_api_mean <- mean(so_api_annual_mean$Abundance)
so_api_sd <- sd(so_api_annual_mean$Abundance)

so_api_z <- (so_api_annual_mean$Abundance - so_api_mean) / so_api_sd

# Pink sealice
PI_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "PI") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance)
            ) %>% 
  mutate_if(is.numeric, round, 2)

ggdensity(PI_api_annual_mean$Abundance)
shapiro.test(PI_api_annual_mean$Abundance)

PI_api_mean <- mean(PI_api_annual_mean$Abundance)
PI_api_sd <- sd(PI_api_annual_mean$Abundance)

PI_api_z <- (PI_api_annual_mean$Abundance - PI_api_mean) / PI_api_sd

# Chum sealice
CU_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "CU") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance)
            ) %>% 
  mutate_if(is.numeric, round, 2)

ggdensity(CU_api_annual_mean$Abundance)
shapiro.test(CU_api_annual_mean$Abundance)

CU_api_mean <- mean(CU_api_annual_mean$Abundance)
CU_api_sd <- sd(CU_api_annual_mean$Abundance)

CU_api_z <- (CU_api_annual_mean$Abundance - CU_api_mean) / CU_api_sd

# Calculate catch intensity z-scores

# Sockeye catch intensity
so_catch_intensity <- catch_intensity %>% 
  filter(species == "Sockeye")

ggdensity(so_catch_intensity$mean_catch)
shapiro.test(so_catch_intensity$mean_catch)

so_mean_catch_intensity <- mean(so_catch_intensity$mean_catch)
so_sd_catch_intensity <- sd(so_catch_intensity$mean_catch)

so_catch_intensity <- so_catch_intensity %>% 
  mutate(z_score = (mean_catch - so_mean_catch_intensity) / so_sd_catch_intensity)

so_catch_intensity_z <- so_catch_intensity$z_score

# Pink catch intensity
pi_catch_intensity <- catch_intensity %>% 
  filter(species == "Pink")

ggdensity(pi_catch_intensity$mean_catch)
shapiro.test(pi_catch_intensity$mean_catch)

pi_mean_catch_intensity <- mean(pi_catch_intensity$mean_catch)
pi_sd_catch_intensity <- sd(pi_catch_intensity$mean_catch)

pi_catch_intensity <- pi_catch_intensity %>% 
  mutate(z_score = (mean_catch - pi_mean_catch_intensity) / pi_sd_catch_intensity)

pi_catch_intensity_z <- pi_catch_intensity$z_score

# Chum catch intensity
cu_catch_intensity <- catch_intensity %>% 
  filter(species == "Chum")

ggdensity(cu_catch_intensity$mean_catch)
shapiro.test(cu_catch_intensity$mean_catch)

cu_mean_catch_intensity <- mean(cu_catch_intensity$mean_catch)
cu_sd_catch_intensity <- sd(cu_catch_intensity$mean_catch)

cu_catch_intensity <- cu_catch_intensity %>% 
  mutate(z_score = (mean_catch - cu_mean_catch_intensity) / cu_sd_catch_intensity)

cu_catch_intensity_z <- cu_catch_intensity$z_score

# SST z-score

# I'm going to filter the time period down to May and June because that's the most relevant period that juvenile salmon are likely to be in the Strait of Georgia

sst_annual_mean <- qu39 %>%
  ungroup() %>% 
  mutate(date = as_date(date)) %>% 
  mutate(month = month(date), year = year(date)) %>% 
  filter(month >= 5 & month <= 6, station == "QU39") %>% 
  select(year, mean_temp) %>% 
  filter(year <= current_year) |> 
  group_by(year) %>% 
  summarise(sd_temp = sd(mean_temp, na.rm = TRUE),
            mean_temp = mean(mean_temp, na.rm = TRUE))

ggdensity(sst_annual_mean$mean_temp)
shapiro.test(sst_annual_mean$mean_temp)

sst_mean <- mean(sst_annual_mean$mean_temp)  
sst_sd <- sd(sst_annual_mean$mean_temp)

sst_z <- (sst_annual_mean$mean_temp - sst_mean) / sst_sd

sst_anomaly_table <- tibble(year = c(2015:current_year),
                            Estimate = round(sst_annual_mean$mean_temp, 2),
                            SD = round(sst_annual_mean$sd_temp, 2),
                            Z = sst_z)

# Put all the z-scores together into a table


mean <- c(f(so_peak_dates$median), so_catch_intensity$mean_catch, round(so_mean_sd$mean, 1), so_api_annual_mean$Abundance,
          f(PI_peak_dates$median), pi_catch_intensity$mean_catch, round(PI_mean_sd$mean, 1), PI_api_annual_mean$Abundance,
          f(CU_peak_dates$median), cu_catch_intensity$mean_catch, round(CU_mean_sd$mean, 1), CU_api_annual_mean$Abundance)

SD <- c(rep(NA, project_years), so_catch_intensity$sd, so_mean_sd$sd, so_api_annual_mean$sd,
        rep(NA, project_years), pi_catch_intensity$sd, PI_mean_sd$sd, PI_api_annual_mean$sd,
        rep(NA, project_years), cu_catch_intensity$sd, CU_mean_sd$sd, CU_api_annual_mean$sd)

#TODO: Maybe remove this section?? seems like it isn't used anywhere
# ts_mean <- rep(c(f(so_mean_of_median_date), so_mean_catch_intensity,
#                  round(so_avg_length[2], 1), so_api_mean,
#                  f(PI_mean_of_median_date), pi_mean_catch_intensity, 
#                  round(PI_avg_length[2], 1), PI_api_mean,
#                  f(CU_mean_of_median_date), cu_mean_catch_intensity, 
#                  round(CU_avg_length[2], CU_api_mean), 1), each= project_years)

Z <- c(so_migration_dates_z, so_catch_intensity_z, so_length_z, so_api_z, 
       PI_migration_dates_z, pi_catch_intensity_z, PI_length_z, PI_api_z, 
       CU_migration_dates_z, cu_catch_intensity_z, CU_length_z, CU_api_z) 

n_species <- 3 # sockeye, pink, chum
n_parameters <- 4 # migration timing, sealice, catch intensity, condition


#TODO: figure out why there's only 60 of mean and sd and Z
heatmap_data <- tibble(year = rep(c(2015:current_year), n_species* n_parameters),
                       Estimate = mean, SD = SD, Z = Z)
                      
heatmap_data <- rbind(heatmap_data, sst_anomaly_table) %>% 
 mutate(Z = round(Z, 2),
        SD = round(SD, 2)) 

vec <- rep(rep(c("Migration Timing", "Catch Intensity", "Length", "Sea lice Abundance"), each = project_years), n_species)
vec2 <- rep(c("Sea-surface Temperature"), project_years)
vec <- c(vec, vec2)
heatmap_data$measure <- vec

spp_vec <- rep(c("Sockeye", "Pink", "Chum"), each = project_years * n_parameters)
vec3 <- rep(c("Northern Strait of Georgia"), project_years)
spp_vec <- c(spp_vec, vec3)

heatmap_data$spp <- spp_vec 

heatmap_data <- heatmap_data %>%
  select(year, Estimate, SD, Z, measure, spp)


write_csv(heatmap_data, here("supplemental_materials", "report_data", "heatmap_data.csv"))
saveRDS(heatmap_data, here("supplemental_materials", "report_data", "heatmap_data.RDS"))

beepr::beep(3)
```
