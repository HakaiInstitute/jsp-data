---
title: "data wrangle for Hakai Juvenile Salmon Program Time Series"
author: "Brett Johnson"
date: "October 16, 2018"
output: html_document
---

# Summary

This script is meant to ingest all the relevant sources of data that the Juvenile salmon program produces and filters, structures, and processes the data into tidy tables that permit inter-annual comparison of the key migration variables. This script creates the tables that are used in our annual report for the North Pacific Anadromous Fisheries Commission Document.

The key migration variables are migration timing, fish length, sea lice, catch intensity, species proportions, and 30 m depth integrated ocean temperatures. All these parameters are then plotted in a heatmap to provide a snapshot of each seasons variables in comparison to previous years as a hypothesis generating tool.

The data folder is populated manually by exporting .csv files from our Master JSP Time Series google sheet. Not all of these tables are read into this report but remain for historical reference. After the data are filtered and wrangled they are saved to the processed_data folder which is what the Migration Observations Report reads from. 

```{r setup}
library(tidyverse)
library(lubridate)
library(knitr)
library(here)
library(car)
library(googledrive)

# I use this function to produce nice looking dates for plotting
f <- function(x) {
  format(x + as.Date("2019-01-01") -1, format = "%B %d")
}

sites <- read_csv(here("data","sites.csv"))

survey_data <- read_csv(here("data", "survey_data.csv")) %>% 
  # creates a sampling week that starts on the first day of sampling in 2015 
  mutate(sampling_week = as.numeric((yday(survey_date) + 4) %/% 7)) %>% 
  left_join(select(sites,
                   site_id,
                   site_name,
                   region,
                   zone),
            by = "site_id")

# This represents the middle of each sampling week with the midpoint of that week
survey_data$sampling_week <- recode_factor(survey_data$sampling_week, 
                                           `18` = "May 5",
                                           `19` = "May 12" ,
                                           `20` = "May 19", 
                                           `21` = "May 26",
                                           `22` = "June 2",
                                           `23` = "June 9", 
                                           `24` = "June 16", 
                                           `25` = "June 23",
                                           `26` = "June 30", 
                                           `27` = "July 6", 
                                           `28` = "July 13",
                                           .default = "OUT")
survey_data <- survey_data %>% 
  filter(sampling_week != "OUT") # Removes time periods inconsistent between years

seine_data <- read_csv(here("data", "seine_data.csv"))

survey_seines <- left_join(survey_data, seine_data, by = "survey_id") %>% 
  mutate(year = year(survey_date)) %>% 
  drop_na(survey_id) %>% 
  mutate(yday = yday(survey_date),
         year = year(survey_date)) %>%
  # remove ad-hoc collections from migration timing calcs & sampling events from normal sampling period (May 1 - July 9) and only select sites that were collected in every year, or atleast represent the various regions in an assumedly similar way
  filter(
    collection_protocol == "SEMSP",
    set_type == "targeted",
    yday < 190,
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D35",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
      )
    )

write_csv(survey_seines, here::here("processed_data", "survey_seines.csv"))
saveRDS(survey_seines,  here::here("processed_data", "survey_seines.RDS"))

fish_field_data <- read_csv(here("data", "fish_field_data.csv"), guess_max = 20000)

survey_seines_fish <- left_join(survey_seines, fish_field_data, by = "seine_id") %>% 
  drop_na(ufn)

current_year <- max(survey_seines$year, na.rm = T)
project_years <- current_year - 2015 + 1
study_range <- paste(2015, "-", current_year)

```

# Sea lice

Sea lice is probably the most complicated set of data to combine largely because we used so many different methods. We counted lice in the field under different protocols, and we counted them in the lab under different protocols as well. Some methods produce a very fine level of resolution when it comes to species and stage ID whereas other methods are more coarse and only focus on larger 'motile' sealice that can be seen easier. 

Our goal is to create a time series of motile sea lice counts that use methods that are reasonably similar enough to warrant combining observations to make interannual comparisons. The lowest common denominator among all years is that motile sea lice where identfied to species. Thus the time series doesn't contain any stage data.

Each year a new table that contains the motile caliigus and lep counts should be added to the time series in a similar way to how the other sea lice tables are combined.

```{r sealice}
# Summarize field counts of motile sea lice only

field_lice <- read_csv(here("data", "sealice_field.csv")) %>% 
  left_join(survey_seines_fish, by = "ufn") %>%
  drop_na(cal_mot_field) %>% 
  mutate(
    motile_caligus_field = rowSums(select(
      ., "cal_mot_field", "cgf_field"
    )),
    motile_lep_field = rowSums(
      select(
        .,
        "lpam_field",
        "lpaf_field",
        "lam_field",
        "laf_field",
        "lgf_field"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    species,
    region,
    survey_date,
    motile_caligus_field,
    motile_lep_field
  )


# To get all lab motiles, you need to join the results from the lab_motile and lab_finescale tables.
# Stages in lab_finescale need to be summed so that they're at the same level of precision as lab_motiles

# This is sealice motile lab data prior to and not inclduing 2020
sealice_lab_mot <- read_csv(here("data", "sealice_lab_mot.csv"))

sealice_lab_fs_mot <- read_csv(here("data", "sample_results", "sealice_lab_fs.csv")) %>%
  #combine stages 1 and 2 or pre adults
  mutate(lpam_lab = lep_pa_m_1 + lep_pa_m_2,
         lpaf_lab = lep_pa_f_1 + lep_pa_f_2,
         cm_lab = cal_pa_m + cal_a_m) %>% 
  select(ufn,
         cm_lab,
         cpaf_lab = cal_pa_f, # renaming variables to be consistent...
         caf_lab = cal_a_f,
         cgf_lab = cal_grav_f,
         ucal_lab = cal_mot_unid,
         lpam_lab,
         lpaf_lab,
         laf_lab = lep_a_f,
         lam_lab = lep_a_m,
         lgf_lab = lep_grav_f,
         ulep_lab = lep_pa_unid)

sealice_lab_motiles <- bind_rows(sealice_lab_mot, sealice_lab_fs_mot)
rm(sealice_lab_mot, sealice_lab_fs_mot) # cleans up intermediate tables

# add catch metadata to sealice counts
lab_lice <- survey_seines_fish %>%
  inner_join(sealice_lab_motiles, by = "ufn") %>%
  drop_na(cm_lab) %>% 
  # lump all sexes and stages of adult caligus
  mutate(
    motile_caligus_lab = rowSums(
      select(., "cm_lab", "cpaf_lab", "caf_lab", "cgf_lab", "ucal_lab"),
      na.rm = T
    ),
    # lump all sexes and stages of adult leps
    motile_lep_lab = rowSums(
      select(
        .,
        "lpam_lab",
        "lpaf_lab",
        "lam_lab",
        "laf_lab",
        "lgf_lab",
        "ulep_lab"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    species,
    region,
    survey_date,
    motile_caligus_lab,
    motile_lep_lab
  )

# In 2020 the stages were not identified in the lab but just counts of motile leps, or caligus  were conducted. So I just downloaded the google sheet manually to jsp-data and import that table here separately and combine appropriately.
sealice_2020 <- read_csv(here("data", "sealice_lab_mots_2020.csv")) %>%
  rename(motile_caligus_lab = cal_count, motile_lep_lab = lep_count) %>% 
  left_join(survey_seines_fish) %>% 
  select(ufn, site_id, species, region, survey_date, motile_caligus_lab, 
         motile_lep_lab)

lab_lice <- bind_rows(lab_lice, sealice_2020)
rm(sealice_lab_motiles, sealice_2020)


sealice_time_series <- full_join(lab_lice, field_lice) %>%
  # with preference to lab ID, combine field and lab ID columns into one column
  mutate(motile_caligus = coalesce(motile_caligus_lab, motile_caligus_field)) %>%
  mutate(motile_lep = coalesce(motile_lep_lab, motile_lep_field)) %>%
  mutate(all_lice = motile_caligus + motile_lep) %>%
  select(ufn,
         survey_date,
         site_id,
         region,
         species,
         motile_lep,
         motile_caligus,
         all_lice) %>%
   gather(`motile_caligus`,
         `motile_lep`,
         `all_lice`,
         key = louse_species,
         value = n_lice) %>%
  mutate(year = year(survey_date)) %>% 
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D35",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  drop_na() %>%
  filter(species %in% c("SO", "PI", "CU"))

rm(lab_lice, field_lice)

write_csv(sealice_time_series, here("processed_data", "sealice_time_series.csv"))
saveRDS(sealice_time_series,  here::here("processed_data", "sealice_time_series.RDS"))

# Calculate sea lice statistics that get reported
abundance_df <- sealice_time_series %>% 
  group_by(year, region, louse_species, species) %>% 
  summarize(abundance =  mean(n_lice, na.rm = T), 
            sd = sd(n_lice, na.rm = T),
            n = n()) %>% 
  mutate(se = sd / sqrt(n),
         abundance_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Abundance = abundance) %>%  
  # mutate("Abundance, 95% CI" = paste(Abundance, "+/-", lower_ci)) %>% 
  select(year, region, species, louse_species, n, Abundance, abundance_ci)

prevalence <- sealice_time_series %>%
  mutate(lousy = ifelse(n_lice > 0, "has_lice", "no_lice")) %>% 
  group_by(year, region, louse_species, species, lousy) %>% 
  summarize(n = n()) %>% 
  spread(key = lousy, value = n) %>% 
  mutate(n = sum(has_lice, no_lice), prevalence = has_lice / n) %>% 
  drop_na() %>% 
  do(broom::tidy(binom.test(.$has_lice, .$n, 0.5, alternative = "two.sided",
                            conf.level = 0.95))) %>% 
  rename("prevalence" = "estimate") %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Prevalence = prevalence, n = parameter, prevalence_ci = conf.low) %>% 
  select(year, region, species, louse_species, n, Prevalence, prevalence_ci)

intensity <- sealice_time_series %>%
  filter(n_lice > 0) %>%
  select(year, region,  louse_species, species, n_lice) %>%
  group_by(year, region, species, louse_species) %>%
  summarise(intensity =  mean(n_lice, na.rm = T), 
            sd = sd(n_lice, na.rm = T),
            n = n()) %>% 
  mutate(se = sd / sqrt(n),
         intensity_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Intensity = intensity) %>% 
  select(year, region, species, louse_species, Intensity, intensity_ci)

sealice_summary_table <- full_join(abundance_df, prevalence) %>% 
  full_join(intensity) %>% 
  rename(Year = year, Region = region, Species = species) %>% 
  mutate(Year = as.character(Year))

write_csv(sealice_summary_table, here("processed_data", "sealice_summary_table.csv"))
saveRDS(sealice_summary_table,  here::here("processed_data", "sealice_summary_table.RDS"))
```

```{r sealice bootstrapping abundance}
library(rsample)
library(dplyr)
library(tidyr)
library(broom)
library(purrr)
library(modelr)

mean_n_lice <- function(splits) {
  x <- analysis(splits)
  mean(x$n_lice)
}

# this_is_it correctly nests the data into its hierarchical form
this_is_it <- sealice_time_series %>% 
  filter(louse_species != "all_lice") %>% 
  split(list(.$year, .$region, .$site_id, .$survey_date, .$species, .$louse_species))

# remove empty permutations
row_lt1 <- which(sapply(this_is_it, nrow) < 1)
this_is_it <- this_is_it[-row_lt1]

# Written by Biljana
boots <- lapply(1:length(this_is_it), function(x){bootstraps(this_is_it[[x]], times=1000)})
 
abund_boot_samples <- lapply(1:length(boots),function(x){  
  boots[[x]] %>% 
  mutate(samples = unlist(map(splits, mean_n_lice)))})

extract.boot.CI <- lapply(1:length(abund_boot_samples), function(x){quantile(abund_boot_samples[[x]]$samples,c(0.025, 0.975))} )

extract.boot.CI = do.call(rbind,extract.boot.CI)

# Back to Brett's code
extract.boot.CI <- as_tibble(extract.boot.CI) 
extract.boot.CI$estimate <- (extract.boot.CI$`97.5%` + extract.boot.CI$`2.5%`) / 2
category <- names(this_is_it)
extract.boot.CI$category <- category

lice_bootstraps <- extract.boot.CI %>% 
separate(category, into = c("year", "region", "site_id", "year2", "month", "day", "species", "stage", "louse_species")) %>% 
  select(-year, year = year2)

readr::write_csv(lice_bootstraps, here("processed_data", "lice_bootstraps.csv"))
saveRDS(lice_bootstraps,  here::here("processed_data", "lice_bootstraps.RDS"))
beepr::beep(8)
```

```{r Catch Intensity}
# In 2015 and 2016 we only enumerated seines with sockeye. In 2017 onwards we enumerated catch of all seines even if they didn't have sockeye. This created a problem because from those first two years of the program we only have pink and chum abundance measurements when they were caught with sockeye. So the only inter-annual comparable abundance parameter across years is 'intensity'— the number of sockeye, pink, and chum that were caught when greater than zero sockeye were caught.

catch_intensity <- survey_seines %>% 
  rename("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total") %>%
  # remove seines that did not catch any sockeye, even if they caught other spp
  filter(Sockeye > 0) %>% 
  select(year, Sockeye, Pink, Chum) %>% 
  gather(key = species, value = catch, - year) %>% 
  # Filter out catches where no pink and chum were caught, so that catch intensity is consistent for each species. Ie. catch intensity for sockeye is the average number of sockeye when catch of sockeye is > 0, catch intensity of pink is the average number of pink caught when > 1 pink was caught, and so on for chum as well.
  filter(catch > 0)

catch_intensity <- catch_intensity %>% 
  group_by(year, species) %>% 
  summarize(mean_catch = mean(catch),
            sd = sd(catch),
            n = n())%>% 
  mutate(se = sd / sqrt(n),
         lower_ci = qt(1 - (0.05 / 2), n - 1) * se,
         upper_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 1)

write_csv(catch_intensity, here::here("processed_data", "catch_intensity.csv"))
saveRDS(catch_intensity,  here::here("processed_data", "catch_intensity.RDS"))
```

```{r Migration Timing}

# Take the average of surveys with double effort in 2019

dbl_effort_2019 <- survey_seines %>% 
  distinct(survey_id, .keep_all = TRUE) %>% 
  filter(survey_date > as.Date("2019-01-01") & survey_date < ("2019-12-31")) %>% 
  mutate(week = isoweek(survey_date),
         site_week = paste(site_id,week,sep="-")) %>% 
  group_by(site_week) %>% 
  mutate(count=n()) %>% 
  filter(count>1) %>% 
  mutate_at(c("so_total", "pi_total", "cu_total", "co_total", "he_total"), mean) %>% 
  mutate_at(c("so_total", "pi_total", "cu_total", "co_total", "he_total"), funs(round(.,0)))

# I first take Cumulative abundance
tidy_catch <- survey_seines %>%
  # Filter out the double-effort surveys
  filter(!survey_id %in% dbl_effort_2019$survey_id)

dbl_effort_2019 <- dbl_effort_2019 %>% 
  # Remove secondary surveys so only one record remains that contains the averaged catch values
  distinct(site_week, .keep_all = TRUE) %>% 
  ungroup() %>% 
  select(-site_week)

tidy_catch <- tidy_catch %>% 
  bind_rows(dbl_effort_2019) %>% 
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # filter out instances when sockeye were not caught because in 2015 we only enumerated sets that had sockeye in them, and moving forward we plan on enumerating all sets. If we included sets that had zero sockeye in 2017 and 2018 then our CPUE of sockeye in 2015 and 2016 would be biased high. Therefore these abundance metrics are strictly based on seines which had sockeye and we are then measuring how many of each species are in a seine, from seines with sockeye, as our metric of all species abundance and proportion. This is the only way to resolve inconsistent sampling strategies between years.
  filter(so_total > 0) %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

tidy_catch <- as.data.frame(tidy_catch)

tidy_catch <- tidy_catch %>%
  mutate(yday = yday(survey_date))

# TIME SERIES AVERAGES

# Create a table with time series average daily proportion for each species and each region for all years combined 

#Sockeye
so_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "so_total", year != current_year) %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

so_total_catch_expanded_cum_DI <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

so_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    so_total_catch_expanded_cum_DI$percent,
    so_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

#Pink
pi_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "pi_total", year != current_year) %>%
  # figure out if a year is odd or even and filter out odd years
  mutate(even_or_odd = year %% 2) %>% 
  filter(even_or_odd != 1) %>% 
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

pi_total_catch_expanded_cum_DI <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

pi_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    pi_total_catch_expanded_cum_DI$percent,
    pi_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Chum
cu_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "cu_total", year != current_year) %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

cu_total_catch_expanded_cum_DI <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

cu_total_predict_average_prop_DI <-
  hakaisalmon::log_cumul_abund(
    cu_total_catch_expanded_cum_DI$percent,
    cu_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create table of time series average migration timing for DI
predict_average_prop <-
  rbind(
    pi_total_predict_average_prop_DI,
    so_total_predict_average_prop_DI,
    cu_total_predict_average_prop_DI
  ) %>%
  drop_na(daily_percent)

predict_average_prop$species <- factor(predict_average_prop$species) %>% 
  fct_recode("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total")

write_csv(predict_average_prop,
          here::here("processed_data", "predict_average_prop.csv"))
saveRDS(predict_average_prop, here::here("processed_data", "predict_average_prop.RDS"))

# ANNUAL CUMULATIVE ABUNDANCE CURVES FOR EACH SPECIES

# Discovery Islands only, not doing this for Johnstone Strait
daily_mean_cumul_abund_annual <- tidy_catch %>%
  ungroup() %>%
  filter(region == "DI") %>%
  mutate(year = as.factor(year)) %>%
  group_by(year, region, species) %>%
  arrange(year, region, species, yday) %>% 
  mutate(cumsum = cumsum(n),
          percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    survey_date = yday,
    region = region,
    species = species,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  group_by(year, region, species, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

#2015 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2015)

so_predict_annual_prop_DI_2015 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2015$percent,
                  so_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# chum
cu_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2015)

cu_predict_annual_prop_DI_2015 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2015$percent,
                  cu_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2016 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2016)

so_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2016$percent,
                  so_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2016)

pi_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2016$percent,
                  pi_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2016)

cu_predict_annual_prop_DI_2016 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2016$percent,
                  cu_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2017 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2017)

so_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2017$percent,
                  so_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "SO")


## chum
cu_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2017)
          
cu_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
                  cu_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

cu_cum_abund_annual_DI_2017$species <- "Chum"

# For now, I'll fjust use raw points without a line
# TODO: Consider removing this data frame, depending on further model selection
write_csv(cu_cum_abund_annual_DI_2017, here("processed_data", "cu_cum_abund_DI_2017.csv"))
saveRDS(cu_cum_abund_annual_DI_2017, here::here("processed_data", "cu_cum_abund_annual_DI_2017.RDS"))          

cu_predict_annual_prop_DI_2017 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
                  cu_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

cu_cum_abund_2017 <- cu_cum_abund_annual_DI_2017 %>% 
  ungroup() %>% 
  mutate(species = "Chum")

# For now, I'll fjust use raw points without a line
write_csv(cu_cum_abund_2017, here("processed_data", "cu_cum_abund_2017.csv"))
saveRDS(cu_cum_abund_2017, here::here("processed_data", "cu_cum_abund_2017.RDS"))

#2018 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2018)

so_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2018$percent,
                  so_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2018)

pi_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2018$percent,
                  pi_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2018)

cu_predict_annual_prop_DI_2018 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2018$percent,
                  cu_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2019 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2019)

so_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2019$percent,
                  so_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2019)

pi_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2019$percent,
                  pi_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2019 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2019)

cu_predict_annual_prop_DI_2019 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2019$percent,
                  cu_cum_abund_annual_DI_2019$survey_date) %>%
  mutate(region = "DI", year = 2019) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2020 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2020)

so_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(so_cum_abund_annual_DI_2020$percent,
                  so_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2020)

pi_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(pi_cum_abund_annual_DI_2020$percent,
                  pi_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2020 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2020)

cu_predict_annual_prop_DI_2020 <-
  hakaisalmon::log_cumul_abund(cu_cum_abund_annual_DI_2020$percent,
                  cu_cum_abund_annual_DI_2020$survey_date) %>%
  mutate(region = "DI", year = 2020) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

# Put each years modelled cumulative abundnance into one table
predict_annual_prop <-
  rbind(
    so_predict_annual_prop_DI_2015,
    cu_predict_annual_prop_DI_2015,
    so_predict_annual_prop_DI_2016,
    pi_predict_annual_prop_DI_2016,
    cu_predict_annual_prop_DI_2016,
    so_predict_annual_prop_DI_2017,
    cu_predict_annual_prop_DI_2017, 
    so_predict_annual_prop_DI_2018,
    pi_predict_annual_prop_DI_2018,
    cu_predict_annual_prop_DI_2018,
    so_predict_annual_prop_DI_2019,
    pi_predict_annual_prop_DI_2019,
    cu_predict_annual_prop_DI_2019,
    so_predict_annual_prop_DI_2020,
    pi_predict_annual_prop_DI_2020,
    cu_predict_annual_prop_DI_2020
  )

predict_annual_prop$species <- factor(predict_annual_prop$species) %>% 
  fct_recode("Sockeye" = "SO", "Pink" = "PI", "Chum" = "CU") 

colnames(predict_annual_prop)[1] <- "x"

write_csv(predict_annual_prop, here::here("processed_data", "predict_annual_prop.csv"))
saveRDS(predict_annual_prop, here::here("processed_data", "predict_annual_prop.RDS"))

# Create data table for real (not modelled), q1, q2, q3 migration timing stats
peak_dates <- tidy_catch[rep(row.names(tidy_catch), tidy_catch$n), 1:6] %>%
  filter(species %in% c("so_total", "pi_total", "cu_total")) %>%
  mutate(yday = yday(survey_date)) %>%
  group_by(year, region, species) %>%
  summarise(n = n(), q1 = quantile(yday, probs = 0.25), q3 = quantile(yday, probs = 0.75), median = median(yday)) %>%
  ungroup() %>% 
  mutate(species = replace(species, species == "so_total", "SO")) %>%
  mutate(species = replace(species, species == "pi_total", "PI")) %>%
  mutate(species = replace(species, species == "cu_total", "CU")) %>% 
  mutate(Spread = median - q1) %>% 
  mutate(q1 = ifelse(species == "PI" & year %% 2 == 1, NA, q1),
         median = ifelse(species == "PI" & year %% 2 == 1, NA, median),
         q3 = ifelse(species == "PI" & year %% 2 == 1, NA, q3),
         Spread = ifelse(species == "PI" & year %% 2 == 1, NA, Spread)) %>% 
    mutate(year = as.character(year))

tsa_peak_dates <- peak_dates %>%
  ungroup() %>% 
  group_by(region, species) %>% 
  summarise(n = mean(n, na.rm = TRUE),
            q1 = mean(q1, na.rm = TRUE),
            median = mean(median, na.rm = TRUE),
            q3 = mean(q3, na.rm = TRUE),
            Spread = mean(Spread, na.rm = TRUE)
            ) %>% 
  mutate(year = paste(2015, "-", current_year)) %>% 
  select(year, everything())

peak_dates <- bind_rows(tsa_peak_dates, peak_dates)

write_csv(peak_dates, here("processed_data","peak_dates.csv"))
saveRDS(peak_dates, here::here("processed_data", "peak_dates.RDS"))
```

```{r Lengths and Condition}

fish_lab_data <- read_csv(here("data", "fish_lab_data.csv"), guess_max = 10000)

fish_data <- left_join(survey_seines_fish, fish_lab_data, by = "ufn") %>%
  # combine both fork length measurements
  mutate(
    fork_length_lab = as.numeric(fork_length),
    fork_length_field = as.numeric(fork_length_field),
    fork_length = coalesce(fork_length_lab, fork_length_field)
  )

length_histo <- fish_data %>%
  select(survey_date, region, species, fork_length) %>%
  drop_na(fork_length) %>%
  mutate(year = year(survey_date)) %>%
  # Remove incidtentaly sampled species
  filter(species != "CK", species != "CO", species != "HE") %>%
  mutate(year = as.factor(year))

saveRDS(length_histo, here::here("processed_data", "length_histo.RDS"))
write_csv(length_histo, here::here("processed_data", "length_histo.csv"))

#TODO: Filter lengths and weights to only fish from between May 1 and July 9

fish_cond <- fish_data %>% 
  select(survey_date, species, weight, fork_length) %>% 
  filter(species %in% c('SO', 'PI', 'CU', 'CO')) %>% 
  mutate(year = factor(year(survey_date))) 

write_csv(fish_cond, here("processed_data", "fish_cond.csv"))
saveRDS(fish_cond, here("processed_data", "fish_cond.RDS"))
```

```{r Species Proportions}


spp_prop <- survey_seines %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # Next I remove instances when no sockeye were caught. I'm doing this because in 2015 and 2016 we only enumerated catches in which we caught sockeye, whereas in 2017 and 2018 we enumerated all seines. So to reduce the bias introduced from the field method i filter the comparison down to where field methods are comparable
  filter(so_total > 0) %>%
  # remove instances when not all species were enumerated by droping rows with NA
  drop_na() %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

spp_prop_expanded <-
  spp_prop[rep(row.names(spp_prop), spp_prop$n), 1:6] %>%
  mutate(yday = yday(survey_date), year = year(survey_date))

proportions <- spp_prop_expanded %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n)) %>%
  ungroup()

write_csv(proportions, here::here("processed_data", "proportion.csv"))
saveRDS(proportions, here::here("processed_data", "proportion.RDS"))
```

```{r SST}
# Get CTD data from EIMS database using R API

# Run this line independently, check console for URL, and authorize
client <- hakaiApi::Client$new()

# You have to break up the data request into a few chunks and then combine because the API has a limit on how much data you can assess at once.
qu39_endpoint_pre_2018 <- sprintf("%s/%s", client$api_root, "ctd/views/file/cast/data?station=QU39&start_dt>=2015-02-01&start_dt<=2018-08-01&limit=-1")

qu39_pre_2018 <- client$get(qu39_endpoint_pre_2018) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt),
    week = week(start_dt)
  ) %>% 
  select(-weather) # removing column with different names in pre and post 2018 files

qu39_endpoint_post_2018 <- sprintf("%s/%s", client$api_root, "ctd/views/file/cast/data?station=QU39&start_dt>2018-08-01&limit=-1")

qu39_post_2018 <- client$get(qu39_endpoint_post_2018) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt),
    week = week(start_dt)
  ) %>% 
  select(-c(weather.sky_cover, weather.wind_speed, weather.swell_height)) # removing columns that don't exist in pre_2018 file.

#make sure column names are the same and in the same order before binding rows
qu39_all <- bind_rows(qu39_pre_2018, qu39_post_2018) %>% 
  filter(yday > 32, yday < 213, depth <= 30)

qu39 <- qu39_all %>% 
  select(
    year,
    date,
    week,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, date, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  ) %>% 
  ungroup()

write_csv(qu39, here::here("processed_data", "ctd_all.csv"))
saveRDS(qu39, here::here("processed_data", "ctd_all.RDS"))


# SST ANOMALY DATA
## Create current year data to compare to time series which excludes the current year for comparison purposes

qu39_this_year <- qu39 %>%
  ungroup() %>% 
  filter(year(date) == current_year)

qu39_average <- qu39 %>%
  filter(year(date) != current_year)

# Here I generate a loess regression for the average temperature based on the time series. Loess and the span variable are both rather arbitrary and more of s statistical convenience than perhaps a more rigorous model?
temp.lo_qu39 <-
  loess(mean_temp ~ yday, qu39_average, SE = T, span = 0.35)

#create table for predicitions from loess function
sim_temp_data_qu39 <-
  tibble(yday = seq(min(qu39_average$yday), max(qu39_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu39$predicted_mean_temp <-
  predict(temp.lo_qu39, sim_temp_data_qu39, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line so that colour of plot will change above and below the trend line.
# This code will have to be re-written manually each yeat to interpolate the mid points for when current year trend line crosses over the trendline
# See https://stackoverflow.com/questions/27135962/how-to-fill-geom-polygon-with-different-colors-above-and-below-y-0
qu39_temp_anomaly_data <-
  left_join(sim_temp_data_qu39, qu39_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>% # run up to just this line to see where diff changes from pos to neg and then add a row that is the will be the mid point between the two days and add the predicted value from the loess regression and then run the whole chunk
   add_row(
    station = "QU39",
    yday = (108 + 120) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (108 + 120) / 2),
    mean_temp = predict(temp.lo_qu39, (108 + 120) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (135 + 147) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (135 + 147) / 2),
    mean_temp = predict(temp.lo_qu39, (135 + 147) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (147 + 156) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (147 + 156) / 2),
    mean_temp = predict(temp.lo_qu39, (147 + 156) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (177 + 182) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (177 + 182) / 2),
    mean_temp = predict(temp.lo_qu39, (177 + 182) / 2)
  ) %>% 
  add_row(
    station = "QU39",
    yday = (182 + 191) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (182 + 191) / 2),
    mean_temp = predict(temp.lo_qu39, (182 + 191) / 2)
  ) %>% 
  add_row(
    station = "QU39",
    yday = (197 + 204) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (197 + 204) / 2),
    mean_temp = predict(temp.lo_qu39, (197 + 204) / 2)
  ) %>% 
  add_row(
    station = "QU39",
    yday = (204 + 211) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (204 + 211) / 2),
    mean_temp = predict(temp.lo_qu39, (204 + 211) / 2)
  )

# Create min and max temp value for any given week of the time series
qu39_min_max <- qu39_all %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature), week = week) %>%
  ungroup() %>%
  group_by(week) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU39")



min_max_data <- qu39_min_max
saveRDS(min_max_data,  here::here("processed_data", "min_max_temps.RDS"))
write_csv(min_max_data,  here::here("processed_data", "min_max_temps.csv"))

average_temps <- qu39_average
saveRDS(average_temps, here::here("processed_data", "average_temps.RDS"))
write_csv(average_temps, here::here("processed_data", "average_temps.csv"))

temperature_anomaly_data <- qu39_temp_anomaly_data
saveRDS(temperature_anomaly_data,
        here::here("processed_data", "temperature_anomaly_data.RDS"))
write_csv(temperature_anomaly_data,
          here::here("processed_data", "temperature_anomaly_data.csv"))
```

# Heat Map

The idea of the heatmap is to provide one plot that broadly characterizes the variability, or the deviations from the time series average, for each migration parameter. This is done using a z-score which can be thought of as the number of standard deviations away from the mean. This assumes normality in the distribution of means for each annually averaged variable (peak migration date, length, etc). Distributions are checked for normality and if not normal, a non-parametric z score is used.

```{r Heatmap}
# HEATMAP Create heatmaps for key parameters

# Migration Timing z-scores

# Sockeye migrations
so_peak_dates <- peak_dates %>% 
  filter(species == "SO", region == "DI", year != study_range) 

# TODO: check for normality or use non parametric value
so_mean_of_median_date <- mean(so_peak_dates$median)
so_sd_of_median_date <- sd(so_peak_dates$median)

so_peak_dates <- so_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - so_mean_of_median_date) / so_sd_of_median_date)

so_migration_dates_z <- so_peak_dates$z_score

# Pink migrations
PI_peak_dates <- peak_dates %>% 
  filter(species == "PI", region == "DI", year != study_range) 

PI_mean_of_median_date <- mean(PI_peak_dates$median, na.rm = TRUE)
PI_sd_of_median_date <- sd(PI_peak_dates$median, na.rm = TRUE)

 
PI_peak_dates <- PI_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - PI_mean_of_median_date) / PI_sd_of_median_date)

PI_migration_dates_z <- PI_peak_dates$z_score

# Chum migrations
CU_peak_dates <- peak_dates %>% 
  filter(species == "CU", region == "DI", year != study_range) 

CU_mean_of_median_date <- mean(CU_peak_dates$median)
CU_sd_of_median_date <- sd(CU_peak_dates$median)

CU_peak_dates <- CU_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - CU_mean_of_median_date) / CU_sd_of_median_date)

CU_migration_dates_z <- CU_peak_dates$z_score

# Length z-scores

# Sockeye lengths
so_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "SO")

so_avg_length <- as.numeric(so_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

so_sd_length <- as.numeric(so_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

so_length_z <- (so_mean_sd$mean - so_avg_length[2]) / so_sd_length[2]

# Pink lengths
PI_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "PI")

PI_avg_length <- as.numeric(PI_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

PI_sd_length <- as.numeric(PI_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

PI_length_z <- (PI_mean_sd$mean - PI_avg_length[2]) / PI_sd_length[2]

# Chum lengths
CU_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "CU")

CU_avg_length <- as.numeric(CU_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

CU_sd_length <- as.numeric(CU_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

CU_length_z <- (CU_mean_sd$mean - CU_avg_length[2]) / CU_sd_length[2]

# Sealice z-scores

# Sockeye sealice
so_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "SO") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance),
            ) %>% 
  mutate_if(is.numeric, round, 2)

so_api_mean <- mean(so_api_annual_mean$Abundance)
so_api_sd <- sd(so_api_annual_mean$Abundance)

so_api_z <- (so_api_annual_mean$Abundance - so_api_mean) / so_api_sd

# Pink sealice
PI_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "PI") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance)
            ) %>% 
  mutate_if(is.numeric, round, 2)

PI_api_mean <- mean(PI_api_annual_mean$Abundance)
PI_api_sd <- sd(PI_api_annual_mean$Abundance)

PI_api_z <- (PI_api_annual_mean$Abundance - PI_api_mean) / PI_api_sd

# Chum sealice
CU_api_annual_mean <- lice_bootstraps %>% 
  ungroup() %>% 
  filter(louse_species == "caligus") %>% 
  filter(species == "CU") %>% 
  select("Year" = year, "Species" = species, "Abundance" = estimate) %>% 
  group_by(Year) %>% 
  summarize(sd = sd(Abundance),
            Abundance = mean(Abundance)
            ) %>% 
  mutate_if(is.numeric, round, 2)

CU_api_mean <- mean(CU_api_annual_mean$Abundance)
CU_api_sd <- sd(CU_api_annual_mean$Abundance)

CU_api_z <- (CU_api_annual_mean$Abundance - CU_api_mean) / CU_api_sd
# Calculate catch intensity z-scores

# Sockeye catch intensity
so_catch_intensity <- catch_intensity %>% 
  filter(species == "Sockeye")

so_mean_catch_intensity <- mean(so_catch_intensity$mean_catch)
so_sd_catch_intensity <- sd(so_catch_intensity$mean_catch)

so_catch_intensity <- so_catch_intensity %>% 
  mutate(z_score = (mean_catch - so_mean_catch_intensity) / so_sd_catch_intensity)

so_catch_intensity_z <- so_catch_intensity$z_score

# Pink catch intensity
pi_catch_intensity <- catch_intensity %>% 
  filter(species == "Pink")

pi_mean_catch_intensity <- mean(pi_catch_intensity$mean_catch)
pi_sd_catch_intensity <- sd(pi_catch_intensity$mean_catch)

pi_catch_intensity <- pi_catch_intensity %>% 
  mutate(z_score = (mean_catch - pi_mean_catch_intensity) / pi_sd_catch_intensity)

pi_catch_intensity_z <- pi_catch_intensity$z_score

# Chum catch intensity
cu_catch_intensity <- catch_intensity %>% 
  filter(species == "Chum")

cu_mean_catch_intensity <- mean(cu_catch_intensity$mean_catch)
cu_sd_catch_intensity <- sd(cu_catch_intensity$mean_catch)

cu_catch_intensity <- cu_catch_intensity %>% 
  mutate(z_score = (mean_catch - cu_mean_catch_intensity) / cu_sd_catch_intensity)

cu_catch_intensity_z <- cu_catch_intensity$z_score

# SST z-score

# I'm going to filter the time period down to May and June because that's the most relevant period that juvenile salmon are likely to be in the Strait of Georgia

sst_annual_mean <- qu39 %>%
  ungroup() %>% 
  mutate(date = as_date(date)) %>% 
  mutate(month = month(date), year = year(date)) %>% 
  filter(month >= 5 & month <= 6, station == "QU39") %>% 
  select(year, mean_temp) %>% 
  group_by(year) %>% 
  summarise(sd_temp = sd(mean_temp, na.rm = TRUE),
            mean_temp = mean(mean_temp, na.rm = TRUE))

sst_mean <- mean(sst_annual_mean$mean_temp)  
sst_sd <- sd(sst_annual_mean$mean_temp)

sst_z <- (sst_annual_mean$mean_temp - sst_mean) / sst_sd

sst_anomaly_table <- tibble(year = c(2015:current_year),
                            Estimate = round(sst_annual_mean$mean_temp, 2),
                            SD = round(sst_annual_mean$sd_temp, 2),
                            Z = sst_z)

# Put all the z-scores together into a table


mean <- c(f(so_peak_dates$median), so_catch_intensity$mean_catch, round(so_mean_sd$mean, 1), so_api_annual_mean$Abundance,
          f(PI_peak_dates$median), pi_catch_intensity$mean_catch, round(PI_mean_sd$mean, 1), PI_api_annual_mean$Abundance,
          f(CU_peak_dates$median), cu_catch_intensity$mean_catch, round(CU_mean_sd$mean, 1), CU_api_annual_mean$Abundance)

SD <- c(rep(NA, project_years), so_catch_intensity$sd, so_mean_sd$sd, so_api_annual_mean$sd,
        rep(NA, project_years), pi_catch_intensity$sd, PI_mean_sd$sd, PI_api_annual_mean$sd,
        rep(NA, project_years), cu_catch_intensity$sd, CU_mean_sd$sd, CU_api_annual_mean$sd)

ts_mean <- rep(c(f(so_mean_of_median_date), so_mean_catch_intensity,
                 round(so_avg_length[2], 1), so_api_mean,
                 f(PI_mean_of_median_date), pi_mean_catch_intensity, 
                 round(PI_avg_length[2], 1), PI_api_mean,
                 f(CU_mean_of_median_date), cu_mean_catch_intensity, 
                 round(CU_avg_length[2], CU_api_mean), 1), each= project_years)

Z <- c(so_migration_dates_z, so_catch_intensity_z, so_length_z, so_api_z, 
       PI_migration_dates_z, pi_catch_intensity_z, PI_length_z, PI_api_z, 
       CU_migration_dates_z, cu_catch_intensity_z, CU_length_z, CU_api_z) 

n_species <- 3 # sockeye, pink, chum
n_parameters <- 4 # migration timing, sealice, catch intensity, condition


#TODO: figure out why there's only 60 of mean and sd and Z
heatmap_data <- tibble(year = rep(c(2015:current_year), n_species* n_parameters),
                       Estimate = mean, SD = SD, Z = Z)
                      
heatmap_data <- rbind(heatmap_data, sst_anomaly_table) %>% 
 mutate(Z = round(Z, 2),
        SD = round(SD, 2)) 

vec <- rep(rep(c("Migration Timing", "Catch Intensity", "Length", "Sea lice Abundance"), each = project_years), n_species)
vec2 <- rep(c("Sea-surface Temperature"), project_years)
vec <- c(vec, vec2)
heatmap_data$measure <- vec

spp_vec <- rep(c("Sockeye", "Pink", "Chum"), each = project_years * n_parameters)
vec3 <- rep(c("Northern Strait of Georgia"), project_years)
spp_vec <- c(spp_vec, vec3)

heatmap_data$spp <- spp_vec 

heatmap_data <- heatmap_data %>%
  select(year, Estimate, SD, Z, measure, spp)


write_csv(heatmap_data, here("processed_data", "heatmap_data.csv"))
saveRDS(heatmap_data, here("processed_data", "heatmap_data.RDS"))

beepr::beep(1)
```
